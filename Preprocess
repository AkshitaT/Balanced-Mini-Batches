import org.canova.api.conf.Configuration;
import org.canova.api.records.reader.RecordReader;
import org.canova.api.split.FileSplit;
import org.canova.nd4j.nlp.reader.TfidfRecordReader;
import org.canova.nd4j.nlp.vectorizer.TfidfVectorizer;
import org.deeplearning4j.datasets.canova.RecordReaderDataSetIterator;
import org.nd4j.linalg.dataset.BalanceMinibatches;
import org.nd4j.linalg.dataset.api.DataSet;
import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;
import org.nd4j.linalg.dataset.api.preprocessor.DataNormalization;
import org.nd4j.linalg.dataset.api.preprocessor.NormalizerMinMaxScaler;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.File;
import java.util.Arrays;
import java.util.List;

public class Preprocess {

    private static Logger log = LoggerFactory.getLogger(Preprocess.class);


    public static void main( String[] args ) throws Exception {

        int batchSize = 100;

        File trainDirectory = new File("Corpus");
        File minibatches = new File("minibatches");
        File minibatchsave = new File("minibatchsave");
        List<String> label = Arrays.asList(trainDirectory.list());
        int numLabels = label.size();


        Configuration config = new Configuration();
        config.setInt(TfidfVectorizer.MIN_WORD_FREQUENCY, 50);
        config.setBoolean(RecordReader.APPEND_LABEL, true);


        log.info("Loading the Tfidf Reader..");
        TfidfRecordReader trainReader = new TfidfRecordReader();
        trainReader.initialize(config, new FileSplit(trainDirectory));
        DataSetIterator iterator = new RecordReaderDataSetIterator(trainReader, batchSize);


        log.info("Normalizing the data..");
        DataNormalization scaler = new NormalizerMinMaxScaler(0,1);
        scaler.fit(iterator);


        int ctr = 0;
        iterator.reset();
        while (iterator.hasNext()) {
            ctr++;
            DataSet next = iterator.next();
            scaler.transform(next);
            log.info("Counter "+ ctr + ": " + iterator.next().getFeatureMatrix().shapeInfoToString());
        }

        iterator.reset();
        BalanceMinibatches balanceMinibatches = BalanceMinibatches.builder()
                .dataSetIterator(iterator)
                .miniBatchSize(batchSize)
                .numLabels(numLabels)
                .rootDir(minibatches)
                .rootSaveDir(minibatchsave)
                .dataNormalization(scaler)
                .build();

        balanceMinibatches.balance();
        scaler.save(new File("mean.bin"),new File("std.bin"));

    }
}
